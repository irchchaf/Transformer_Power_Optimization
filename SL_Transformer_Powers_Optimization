#Import libraries
import numpy as np
import h5py
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from collections import OrderedDict
from sklearn.metrics import mean_squared_error, mean_absolute_error
# Hyperparameters
d_model = 32
num_heads = 4
num_encoder_layers = 2
dropout_rate = 0.1
learning_rate = 0.001
num_epochs = 10
batch_size = 32
num_APs = 16
# PreProcessing data
def manual_normalize(data):
    """
    Manually normalize data to the range [0, 1] using min-max scaling.
    Returns the normalized data, along with the min and max values for later use.
    """
    min_vals = np.min(data, axis=0)
    max_vals = np.max(data, axis=0)
    normalized_data = (data - min_vals) / (max_vals - min_vals + 1e-8)
    return normalized_data, min_vals, max_vals
def process_and_split_data_manual(filename):
    """
        Process data and Create the train/test sets.
    """
    # Load the HDF5 file
    with h5py.File(filename, "r") as hdf_file:
        # Load AP positions
        ap_positions = np.array(hdf_file["AP_positions"])
        # Normalize AP positions
        ap_positions, ap_min, ap_max = manual_normalize(ap_positions)
        # Dictionaries to store train and test data
        train_data = {}
        test_data = {}
        # Process each user count
        for K in hdf_file["data"]:
            # print(f"\nProcessing data for {K} users...")
            user_data = {}
            for key in hdf_file[f"data/{K}"]:
                user_data[key] = np.array(hdf_file[f"data/{K}/{key}"])
            # Original shapes from the dataset
            num_samples = user_data["user_positions"].shape[0]  # Number of samples
            num_users = user_data["user_positions"].shape[1]    # Number of users
            num_APs = ap_positions.shape[0]                     # Number of APs
            # Normalize user positions
            user_positions, user_min, user_max = manual_normalize(user_data["user_positions"])  # Shape: (samples, num_users, 2)
            # Initialize gamma with the correct shape (samples, users, 2 * num_APs + 2)
            gamma = np.zeros((num_samples, num_users, 2 * num_APs + 2))
            # Iterate through each sample and user to populate gamma
            for sample_idx in range(num_samples):
                for user_idx in range(num_users):
                    # AP positions for this user (repeated for each user in the sample)
                    gamma[sample_idx, user_idx, :2 * num_APs] = np.tile(ap_positions.flatten(), 1)
                    # User position for this user
                    gamma[sample_idx, user_idx, 2 * num_APs:] = user_positions[sample_idx, user_idx]  # Store user position
            # Normalize UL and DL powers
            ul_powers, ul_min, ul_max = manual_normalize(user_data["optimal_UL_powers"])  # Shape: (samples, num_users)
            dl_powers, dl_min, dl_max = manual_normalize(user_data["optimal_DL_powers"])  # Shape: (samples, num_users)
            # Concatenate normalized UL and DL powers to form the output
            y = np.hstack([ul_powers, dl_powers])  # Shape: (samples, 2 * num_users)
            # Split data and retain indices
            X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(
                gamma, y, np.arange(num_samples), test_size=0.1, random_state=42
            )
            train_data[K] = {
                "X_train": X_train,
                "y_train": y_train,
            }
            test_data[K] = {
                "X_test": X_test,
                "y_test": y_test,
                "ul_min": ul_min,
                "ul_max": ul_max,
                "dl_min": dl_min,
                "dl_max": dl_max,
            }
    return train_data, test_data
def process_newK_data(filename):
    """
            Process data for the new user loads unseen during the training.
    """
    # Load the HDF5 file
    with h5py.File(filename, "r") as hdf_file:
        # Load AP positions
        ap_positions = np.array(hdf_file["AP_positions"])
        # Normalize AP positions
        ap_positions, ap_min, ap_max = manual_normalize(ap_positions)
        # Dictionaries to store NEW test data
        Ntest_data = {}
        # Process each user count
        for K in hdf_file["data"]:
            print(f"\nProcessing data for {K} users...")
            user_data = {}
            for key in hdf_file[f"data/{K}"]:
                user_data[key] = np.array(hdf_file[f"data/{K}/{key}"])
            # Original shapes from the dataset
            num_samples = user_data["user_positions"].shape[0]  # Number of samples
            num_users = user_data["user_positions"].shape[1]    # Number of users
            num_APs = ap_positions.shape[0]                     # Number of APs
            # Normalize user positions
            user_positions, user_min, user_max = manual_normalize(user_data["user_positions"])  # Shape: (samples, num_users, 2)
            # Initialize gamma with the correct shape (samples, users, 2 * num_APs + 2)
            Ngamma = np.zeros((num_samples, num_users, 2 * num_APs + 2))
            # Iterate through each sample and user to populate gamma
            for sample_idx in range(num_samples):
                for user_idx in range(num_users):
                    # AP positions for this user (repeated for each user in the sample)
                    Ngamma[sample_idx, user_idx, :2 * num_APs] = np.tile(ap_positions.flatten(), 1)
                    Ngamma[sample_idx, user_idx, 2 * num_APs:] = user_positions[sample_idx, user_idx]  # Store user position
            # Normalize UL and DL powers separately
            ul_powers, ul_min, ul_max = manual_normalize(user_data["optimal_UL_powers"])  # Shape: (samples, num_users)
            dl_powers, dl_min, dl_max = manual_normalize(user_data["optimal_DL_powers"])  # Shape: (samples, num_users)
            # Concatenate normalized UL and DL powers to form the output
            Ny = np.hstack([ul_powers, dl_powers])  # Shape: (samples, 2 * num_users)
            Ntest_data[K] = {
                "X_test": Ngamma,
                "y_test": Ny,
                "ul_min": ul_min,
                "ul_max": ul_max,
                "dl_min": dl_min,
                "dl_max": dl_max,
            }
    return  Ntest_data

# Run the function and get the train and test data
filename = "cell_free_mMIMO_data.hdf5"
# The dataset file contains positions of UEs and APs with precomputed optimal UL/DL powers
# Optimal powers are computed using the closed-form solution in the paper
# L. Miretti, et al "Closed-form max-min power control for some cellular and cell-free massive MIMO networks"
train_data, test_data = process_and_split_data_manual(filename)
train_data = OrderedDict(sorted(train_data.items(), key=lambda item: int(item[0])))
test_data = OrderedDict(sorted(test_data.items(), key=lambda item: int(item[0])))
#===================================================================================================================
# Transformer Encoder Class with Attention Mechanism
class PowerAllocationTransformer(nn.Module):
    def __init__(self, d_model, num_heads, num_encoder_layers, num_APs, dropout_rate):
        super(PowerAllocationTransformer, self).__init__()
        # Transformer Encoder with Dropout
        self.encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, nhead=num_heads, dropout=dropout_rate
        )
        self.transformer_encoder = nn.TransformerEncoder(
            self.encoder_layer, num_layers=num_encoder_layers
        )
        self.fc_input = nn.Linear(2 * num_APs + 2, d_model) #
        self.fc_out_ul = nn.Linear(d_model, 1)
        self.fc_out_dl = nn.Linear(d_model, 1)
    def forward(self, x):
        # Transform input to match Transformer model's requirements
        x = self.fc_input(x)  # Shape: (batch_size, num_users, d_model)
        # Rearrange for the transformer (seq_len, batch_size, d_model)
        x = x.permute(1, 0, 2)  # Shape: (num_users, batch_size, d_model)
        x = self.transformer_encoder(x)  # Apply Transformer Encoder
        x = x.permute(1, 0, 2)  # Change back to (batch_size, num_users, d_model)
        # Predict uplink and downlink powers
        ul_powers = self.fc_out_ul(x)  # Shape: (batch_size, num_users, 1)
        dl_powers = self.fc_out_dl(x)  # Shape: (batch_size, num_users, 1)
        ul_powers = torch.sigmoid(ul_powers)
        dl_powers = torch.sigmoid(dl_powers)
        # Reshape and concatenate UL and DL powers
        ul_powers = ul_powers.squeeze(-1) # Shape: (batch_size, num_users)
        dl_powers = dl_powers.squeeze(-1)  # Shape: (batch_size, num_users)
        output = torch.cat((ul_powers, dl_powers), dim=-1)  # Shape: (batch_size, 2 * num_users)
        return output
#===============================================================================================================
# Training the model
# Dataset Class
class GammaPowerDataset(Dataset):
    def __init__(self, gamma, y):
        self.gamma = gamma
        self.y = y
    def __len__(self):
        return self.gamma.shape[0]

    def __getitem__(self, idx):
        return torch.tensor(self.gamma[idx], dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.float32)
# Training and Validation
def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):
    model.to(device)
    best_val_loss = float('inf')
    for epoch in range(num_epochs):
        # Training phase
        model.train()
        train_loss = 0
        for gamma, y in train_loader:
            gamma, y = gamma.to(device), y.to(device)
            optimizer.zero_grad()
            outputs = model(gamma)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_loss /= len(train_loader)
        # Validation phase
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for gamma, y in val_loader:
                gamma, y = gamma.to(device), y.to(device)
                outputs = model(gamma)
                loss = criterion(outputs, y)
                val_loss += loss.item()
        val_loss /= len(val_loader)
        print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
        # Save best model
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), "best_model_powers_prediction.pth")
            print("Saved Best Model!")
# Prepare the Model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = PowerAllocationTransformer(
    d_model=d_model,
    num_heads=num_heads,
    num_encoder_layers=num_encoder_layers,
    num_APs=num_APs,
    dropout_rate=dropout_rate
)
criterion = nn.MSELoss() # Huber Loss can be used if powers distribution is heavily biased
# criterion = nn.HuberLoss()
optimizer = optim.AdamW(model.parameters(), lr=learning_rate)
for K, data in train_data.items():
    print(f"Training for {K} users...")
    # Split training data into train and validation sets (90% train, 10% val)
    Xtrain, Xval, ytrain, yval = train_test_split(
        data["X_train"], data["y_train"], test_size=0.1, random_state=42
    )
    # Create DataLoaders
    train_dataset = GammaPowerDataset(Xtrain, ytrain)
    val_dataset = GammaPowerDataset(Xval, yval)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    # Train the Model
    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)
#===============================================================================================================
# Testing on new values of K
Nfilename = "cell_free_mMIMO_test_newK.hdf5" # This file contains data with new values of K
Ntest_data = process_newK_data(Nfilename)
# Initialize the combined dictionary
alltest_data = {}
# Add data from test_data (users 2 to 10)
for K, data in test_data.items():
    alltest_data[K] = data
# Add data from Ntest_data (users more than 10)
for K, data in Ntest_data.items():
    alltest_data[K] = data
alltest_data = OrderedDict(sorted(alltest_data.items(), key=lambda item: int(item[0])))
#=========================
model.eval()
print("\n===== Test Set Evaluation =====")
for K, data in alltest_data.items():
    K = int(K)
    # Extract test data
    X_testpos = torch.tensor(data["X_test"], dtype=torch.float32).to(device)
    y_test = torch.tensor(data["y_test"], dtype=torch.float32).to(device)
    ul_min = data["ul_min"]
    ul_max = data["ul_max"]
    dl_min = data["dl_min"]
    dl_max = data["dl_max"]
    with torch.no_grad():
        # Normalized predictions
        predictions = model(X_testpos)  # (num_samples, num_users)
        predicted_UL_powers_norm = predictions[:, :K].cpu().numpy()
        predicted_DL_powers_norm = predictions[:, K:].cpu().numpy()
    # Ground truth normalized  powers
    true_UL_powers_norm = y_test[:, :K].cpu().numpy()
    true_DL_powers_norm = y_test[:, K:].cpu().numpy()
    # ----- Normalized metrics -----
    mse_norm_UL = mean_squared_error(true_UL_powers_norm.flatten(),
                                  predicted_UL_powers_norm.flatten())
    mae_norm_UL = mean_absolute_error(true_UL_powers_norm.flatten(),
                                   predicted_UL_powers_norm.flatten())
    mse_norm_DL = mean_squared_error(true_DL_powers_norm.flatten(),
                                     predicted_DL_powers_norm.flatten())
    mae_norm_DL = mean_absolute_error(true_DL_powers_norm.flatten(),
                                      predicted_DL_powers_norm.flatten())
    # ----- Denormalized metrics -----
    predicted_UL_powers = predicted_UL_powers_norm * (ul_max - ul_min) + ul_min
    true_UL_powers = true_UL_powers_norm * (ul_max - ul_min) + ul_min
    predicted_DL_powers = predicted_DL_powers_norm * (dl_max - dl_min) + dl_min
    true_DL_powers = true_DL_powers_norm * (dl_max - dl_min) + dl_min
    mse_denorm_UL = mean_squared_error(true_UL_powers.flatten(),
                                    predicted_UL_powers.flatten())
    mae_denorm_UL = mean_absolute_error(true_UL_powers.flatten(),
                                     predicted_UL_powers.flatten())
    mse_denorm_DL = mean_squared_error(true_DL_powers.flatten(),
                                       predicted_DL_powers.flatten())
    mae_denorm_DL = mean_absolute_error(true_DL_powers.flatten(),
                                        predicted_DL_powers.flatten())
    print(
        f"Users: {K} | Norm MSE UL: {mse_norm_UL:.6f}, Norm MAE UL: {mae_norm_UL:.6f} | Denorm MSE UL: {mse_denorm_UL:.6f}, Denorm MAE UL: {mae_denorm_UL:.6f}")

    print(
        f"           | Norm MSE DL: {mse_norm_DL:.6f}, Norm MAE DL: {mae_norm_DL:.6f} | Denorm MSE DL: {mse_denorm_DL:.6f}, Denorm MAE DL: {mae_denorm_DL:.6f}")
